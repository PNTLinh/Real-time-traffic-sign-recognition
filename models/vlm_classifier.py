from __future__ import annotations
import os
from typing import List, Tuple, Dict, Sequence, Optional
import json
from pathlib import Path

import torch
import numpy as np
from PIL import Image
import cv2

try:
    import open_clip
except Exception:
    raise ImportError("Cáº§n cÃ i: pip install open-clip-torch")

# ---------------------------------------------------------------------
# CONSTANTS
# ---------------------------------------------------------------------

LABELS_VN = [
    "Cáº¥m Ä‘á»— xe", "Cáº¥m dá»«ng Ä‘á»— xe", "Cáº¥m ngÆ°á»£c chiá»u", "Cáº¥m Ã´ tÃ´",
    "Cáº¥m quay Ä‘áº§u", "Cáº¥m ráº½ pháº£i", "Cáº¥m ráº½ trÃ¡i", "Dá»«ng láº¡i",
    "ÄÆ°á»ng khÃ´ng báº±ng pháº³ng", "ÄÆ°á»ng khÃ´ng Æ°u tiÃªn", "ÄÆ°á»ng Æ°u tiÃªn",
    "NgÆ°á»i Ä‘i bá»™", "Tá»‘c Ä‘á»™ 30", "Tá»‘c Ä‘á»™ 40", "Tá»‘c Ä‘á»™ 50",
    "Tá»‘c Ä‘á»™ 60", "Tá»‘c Ä‘á»™ 80", "Tráº» em qua Ä‘Æ°á»ng", "VÃ²ng xuyáº¿n"
]

TEMPLATES_VI = [
    "biá»ƒn bÃ¡o giao thÃ´ng: {}",
    "biá»ƒn bÃ¡o Ä‘Æ°á»ng bá»™ Viá»‡t Nam: {}",
    "áº£nh chá»¥p biá»ƒn bÃ¡o: {}",
]

TEMPLATES_EN = [
    "traffic sign: {}",
    "road sign: {}",
    "photo of a traffic sign: {}",
]


# ---------------------------------------------------------------------
# MAIN CLASS
# ---------------------------------------------------------------------

class VLMClassifier:
    """
    Refactor version:
    - Há»— trá»£ caching text features (tÄƒng tá»‘c máº¡nh)
    - FP16 safe-mode
    - Safe cropping
    - Clean model design (khÃ´ng chá»©a pipeline logic)
    """

    def __init__(
        self,
        model_name: str = "ViT-B-32",
        pretrained: str = "laion2b_s34b_b79k",
        device: Optional[str] = None,
        use_half: bool = True,
        labels: Optional[List[str]] = None,
        templates_vi: Optional[List[str]] = None,
        templates_en: Optional[List[str]] = None,
        cache_path: str = "weights/vlm/text_features.pt",
    ):
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")

        print(f"ðŸ”„ Loading OpenCLIP model: {model_name} ...")
        self.model, _, self.preprocess = open_clip.create_model_and_transforms(
            model_name, pretrained=pretrained, device=self.device
        )
        self.tokenizer = open_clip.get_tokenizer(model_name)

        # safe FP16
        self.use_half = False
        if use_half and self.device.startswith("cuda"):
            try:
                self.model = self.model.half()
                self.use_half = True
                print("âš¡ Using FP16 (safe-mode enabled)")
            except Exception:
                print("âš ï¸ FP16 not supported, fallback to FP32")

        # labels & templates
        self.labels = labels or LABELS_VN
        self.templates_vi = templates_vi or TEMPLATES_VI
        self.templates_en = templates_en or TEMPLATES_EN

        self.cache_path = Path(cache_path)
        self.text_features: Optional[torch.Tensor] = None

        # Load or build text features
        self._init_text_features()

        print(f"âœ… VLM Classifier ready on {self.device}")
        print(f"ðŸ“Š Classes: {len(self.labels)}")

    # -----------------------------------------------------------------
    # TEXT FEATURES
    # -----------------------------------------------------------------

    def _init_text_features(self):
        """Load tá»« cache hoáº·c build má»›i."""
        if self.cache_path.exists():
            print("ðŸ“¦ Loading cached text features ...")
            self.text_features = torch.load(self.cache_path, map_location=self.device)
            return

        print("ðŸ§± Building text features ...")
        self._build_text_features()
        self._save_cache()

    def _save_cache(self):
        self.cache_path.parent.mkdir(parents=True, exist_ok=True)
        torch.save(self.text_features, self.cache_path)
        print(f"ðŸ’¾ Text features cached â†’ {self.cache_path}")

    def _build_text_features(self):
        prompts = []
        for name in self.labels:
            for t in self.templates_vi:
                prompts.append(t.format(name))
            for t in self.templates_en:
                prompts.append(t.format(name))

        tokens = self.tokenizer(prompts).to(self.device)

        with torch.no_grad():
            feats = self.model.encode_text(tokens)
            feats = feats / feats.norm(dim=-1, keepdim=True)

        # average per-class
        k = len(self.templates_vi) + len(self.templates_en)
        per_class = []
        for i in range(len(self.labels)):
            per_class.append(feats[i * k: (i + 1) * k].mean(dim=0))

        self.text_features = torch.stack(per_class)

    # -----------------------------------------------------------------
    # IMAGE ENCODING
    # -----------------------------------------------------------------

    def _pil_to_tensor(self, image: Image.Image) -> torch.Tensor:
        if image.mode != "RGB":
            image = image.convert("RGB")
        t = self.preprocess(image).unsqueeze(0).to(self.device)
        return t.half() if self.use_half else t

    @torch.inference_mode()
    def classify_image(self, image: Image.Image, topk: int = 3) -> Dict:
        img = self._pil_to_tensor(image)
        img_feat = self.model.encode_image(img)
        img_feat = img_feat / img_feat.norm(dim=-1, keepdim=True)

        logits = img_feat @ self.text_features.T
        probs = logits.softmax(dim=-1)

        topk = min(topk, len(self.labels))
        scores, idxs = probs.topk(topk, dim=-1)

        idxs = idxs[0].tolist()
        scores = scores[0].tolist()

        return {
            "pred_label": self.labels[idxs[0]],
            "pred_score": scores[0],
            "topk_labels": [self.labels[i] for i in idxs],
            "topk_scores": scores,
            "logits": logits.cpu().numpy(),
        }

    # -----------------------------------------------------------------
    # CROP + BATCH PREDICT
    # -----------------------------------------------------------------

    def safe_crop(self, img: np.ndarray, x1, y1, x2, y2):
        h, w = img.shape[:2]
        x1 = max(0, int(x1))
        y1 = max(0, int(y1))
        x2 = min(w, int(x2))
        y2 = min(h, int(y2))

        if x2 <= x1 or y2 <= y1:  
            return None

        crop = img[y1:y2, x1:x2]
        if crop.shape[0] < 5 or crop.shape[1] < 5:
            return None

        return Image.fromarray(cv2.cvtColor(crop, cv2.COLOR_BGR2RGB))

    @torch.inference_mode()
    def classify_crops(self, image_bgr: np.ndarray, boxes_xyxy: Sequence):
        crops = []
        for x1, y1, x2, y2 in boxes_xyxy:
            crop = self.safe_crop(image_bgr, x1, y1, x2, y2)
            if crop is not None:
                crops.append(crop)

        if not crops:
            return []

        # batch encode
        batch = torch.cat([self._pil_to_tensor(im) for im in crops], dim=0)
        img_feat = self.model.encode_image(batch)
        img_feat = img_feat / img_feat.norm(dim=-1, keepdim=True)

        logits = img_feat @ self.text_features.T
        probs = logits.softmax(dim=-1)

        out = []
        for i in range(len(crops)):
            scores, idxs = probs[i].topk(3)
            idxs = idxs.tolist()
            scores = scores.tolist()

            out.append({
                "pred_label": self.labels[idxs[0]],
                "pred_score": scores[0],
                "topk_labels": [self.labels[x] for x in idxs],
                "topk_scores": scores,
            })
        return out
